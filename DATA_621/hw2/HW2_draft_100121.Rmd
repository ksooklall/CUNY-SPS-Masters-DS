---
title: "HW2_100121"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(magrittr)
library(cvms)
library(glue)
library(caret)
library(pROC)
```

### 1. Download the classification output data set.

```{r}

df <-read_csv("https://raw.githubusercontent.com/sconnin/621_Business_Analytics/main/HW2_GRP/classification-output-data.csv")

# select columns that we will use for this project

df%<>%select(class, scored.class, scored.probability)

# assess dataframe dimensions, features, and dtypes

str(df)

```

```{r}
dff <- df
```

]
### 2. Use the table() function to get the raw confusion matrix for this scored dataset.


```{r}

# compile the confusion matrix in table form 

table(df$class, df$scored.class)

```

Results reported in the confusion matrix (CM):

Note: rows in matrix diagram represent predicted class; columns represent actual class. 

-   27 true positives (65.7%)
-   5 false positives (2.8%) - fraction of negatives that are classified as positives
-   119 true negative (14.9%)
-   30 false negatives (16.6%) - fraction of positives that are classified as negatives


Reading the CM plot:

-   Middle of each tile includes normalized count (overall percentage) and, beneath it, count.

-   Bottom of each tile includes the column percentage.

-   Right side of tile includes the row percentage

-   The color intensity is based on counts (n)

see: <https://bit.ly/3zNXHoH>

\#\#\#3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of these predictions.

$Accuracy = \frac{TP + TN}{TP + FP +TN +FN}$

Accuracy is a measure (percentage) of correct predictions

```{r}

#First we write a general function that can be repurposed

class_metric <- function(dframe, actual, predicted, threshold, metric){
    true_pos <- sum(dframe[,actual] == 1 & dframe[,predicted] >= threshold)
    true_neg <- sum(dframe[,actual] == 0 & dframe[,predicted] < threshold)
    false_pos <- sum(dframe[,actual] == 0 & dframe[,predicted] >= threshold)
    false_neg <- sum(dframe[,actual] == 1 & dframe[,predicted] < threshold)
    return(true_pos)
    if(metric == "accuracy"){
        return((true_pos + true_neg) / nrow(dframe))
    } else if(metric == "error"){
        return((false_pos + false_neg) / nrow(dframe))
    } else if(metric == "precision"){
        return(true_pos / (true_pos + false_pos))
    } else if(metric == "sensitivity" | metric == "recall"){
        return(true_pos / (true_pos + false_neg))
    } else if(metric == "specificity"){
        return(true_neg / (true_neg + false_pos))
    } else if(metric == "F1"){
        return(2 * true_pos / (2 * true_pos + false_pos + false_neg))
    } else {
        return("oops.")
    }
}
```


Here we build a function to calculate accuracy

```{r}

class_metric(df, "class", "scored.class", 0.5, 'accuracy')

```
]

```{r}
class_metric <- function(dframe, actual, predicted, threshold){
    true_pos <- sum(dframe[,actual] == 1 & dframe[,predicted] >= threshold)
    true_neg <- sum(dframe[,actual] == 0 & dframe[,predicted] < threshold)
    false_pos <- sum(dframe[,actual] == 0 & dframe[,predicted] >= threshold)
    false_neg <- sum(dframe[,actual] == 1 & dframe[,predicted] < threshold)
    return(true_pos)
}
class_metric(df, "class", "scored.class",.05)

```


###4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.

$ClassificationErrorRate = \frac{FP + FN}{TP + FP +TN +FN}$

The error rate is measure (percentage) of incorrect predictions.

```{r}

class_metric(df, "class", "scored.class",.05, 'error')

```
]
\#\#\#5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

$Precision = \frac{TP}{TP + FP}$

```{r}

class_metric(df, "class", "scored.class",.05, 'precision')


```
### verify that our accuracy and error rate sum to 1.
[
```{r}


class_metric(df, "class", "scored.class", 0.5, 'accuracy')+class_metric(df, "class", "scored.class", 0.5, "error")

```

\#\#\#6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

$Sensitivity = \frac{TP}{TP + FN}$

Sensitivity: The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive

```{r}

class_metric(df, "class", "scored.class", 0.5, 'sensitivity')

```

###7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

$Specificity = \frac{TN}{TN + FP}$

Specificity:The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative.

```{r}

class_metric(df, "class", "scored.class", 0.5, 'specificity')


```

###8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.

eqn 1. $F1 = \frac{TP}{TP+\frac{1}{2}(FP+FN)}$

Another way to write this is:

eqn 2. $F1 = 2*\frac{precision*recall}{precision+recall}$

Or

eqn 3. $F1 = 2*\frac{precision*sensitivity}{precision+sensitivity}$

```{r}

class_metric(df, "class", "scored.class", 0.5, 'F1')

    
```

###9. What are the bounds on the F1 score?

Show that the F1 score will always be between 0 and 1. (Hint: If 0 \< 𝑎 \< 1 and 0 \< 𝑏 \< 1 then 𝑎𝑏 \< 𝑎.)

Recall that:

-   precision can range from 0 to 1

-   sensitivity can range from 0 to 1

The F1 score simplifies to $\frac{2TP}{2TP + FP + FN}$. All variables are >= 0. Then $2TP + FP + FN \geq 2TP$. Then $0 \leq F1 \leq 1$.

###10. Write a function that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example)

Your function should return a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.

The ROC curve plots values of the FP rate (FPR) against the TP rate (TPR) for a selected
cutoff value.

The FPR can be calculated as follows:

$FPR = \frac{cumulativesum(FP)}{colsum(FP)}$

The TPR can be calculated similarly:

$TPR = \frac{cumulativesum(TP)}{colsum(TP)}$

Every point on the ROC curve represents a chosen cut-off.To make an ROC 
curve rank all the values (scored.probability) and link each value to the
assessment (class). Keep in mind that the TPR is equivalent to model 
sensitivity while FPR is equivalent to 1-(model specificity).

The AUC (area under the ROC curve) provides a measure of the model's ability to correctly discriminate (separate) between model categories.In other words the \
proportion of TP vs. the proportion of FP.

Our AUC calculation (included below) is drawn from AUC - calculation based on https://bit.ly/3m7zW6k.

$AUC =\sum  \left ( TPR*FPR_{diff} \right )+\frac{TPR_{diff}*FPR}{2}$

Where:
* TPR = true positive rate
* FPR = false positive rate
* $TPR_{diff}$ = the difference between consecutive TPR observations
* $FPR_{diff}$ = the difference between consecutive FPR observations

In this context, our area estimate is an approximation similar to a Riemann sum 
in integral calculus. 

The first entry in the equation is equivalent to a Riemann rectangle. The second 
entry extends this geometry with a triangle "cap" to compensate for curvature 
in the AUC function. Together they form a trapezoid. The area of each
trapeziod included under the curve is then summed to produce a AUC estimate. 


```{r}
roc_fn <- function(dframe, actual, predicted){
  
  res <- list()
  
  threshold <- seq(0, 1, 0.01)
  false_pos <- 1 - vapply(threshold, class_metric, double(1), dframe = dframe, actual = actual, predicted = predicted, metric = "specificity")
  true_pos <- vapply(threshold, class_metric, double(1), dframe = dframe, actual = actual, predicted = predicted, metric = "sensitivity")
  
  to_plot <- data.frame(threshold, false_pos, true_pos)
  
  #Create a plot and append it to res
  #Calculate AUC and append to res
  
  #return(res)
  return(to_plot)
}

to_plot <- roc_fn(df, "class", "scored.probability")

plot(to_plot$false_pos, to_plot$true_pos)

ggplot(to_plot, aes(x = false_pos, y = true_pos)) +
  geom_path()+
  geom_abline(slope=1, intercept = 0, color = "red")
```

###11. Use your created R functions and the provided classification output data set to produce all of the classification metrics discussed above.

```{r}
for(metric in c("accuracy", "error", "precision", "sensitivity", "specificity", "F1")){
    print(paste0(metric, " = ", round(class_metric(df, "class", "scored.probability", 0.5, metric), 4)))
}
```
]

### 12. Investigate the caret package.

Investigate the Caret package. In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions?

```{r}

cfm_caret <- df

cfm_caret$class<-as.factor(cfm_caret$class)
cfm_caret$scored.class<-as.factor(cfm_caret$scored.class)

conf<-confusionMatrix(cfm_caret$scored.class, cfm_caret$class, positive="1", mode = "everything") #note x, y order matters here

#build comparison table


Calculated<-c(accuracy, sensitivity, specificity, precision, F1)
Caret<-c(conf[[3]]['Accuracy'], conf[[4]]['Sensitivity'], conf[[4]]['Specificity'], conf[[4]]['Precision'], conf[[4]]['F1'])

cbind(Caret, Calculated)

```


###13. Investigate the pROC package.

Use it to generate an ROC curve for the data set. How do the results compare with your own functions?

```{r}

par(pty="s")

proc_demo<-roc(df$class, df$scored.probability, plot=TRUE, legacy.axes=TRUE, auc.polygon=TRUE, col="blue", main=" PROC ROC Curve", max.auc.polygon=TRUE,  print.auc=TRUE)

#This will reset the graph boundary to square and graph the x-axis from 0-1 (1-specificity) rather than 1-0 (specificity).


```
