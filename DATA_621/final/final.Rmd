---
title: "Google Merchandise Store"
author: "Kenan Sooklall"
date: "11/29/2021"
output: html_document
---

```{r echo=F, warning=F, include=F}
library(tidyverse)
library(naniar)
library(jsonlite)
library(priceR)
library(lubridate)
library(gridExtra)
library(corrplot)
```

The dataset we are analyzing is a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset. The raw dataset contains 12 columns to predict the transaction revenue per customer. The outcome from this analysis will aid in better use of marketing budgets. There are 2 datasets, big and small. The small dataset is about 1 million transactions and the big one is about 10 times larger. Due to hardware constraint the small dataset is used in this report.

### Preprocessing

The data is a mix of csv and json where the json data are separate columns. That mismatch requires a lot of preprocessing to get the features and target that we need for analyzing. The json columns (device, geoNetwork, totals, trafficSource) are convert from str json into a json object. Then the json object is parsed with jsonlite into vectors. Those vectors are flattened and then join with the original dataframe. This preprocessing step is required for both the training and testing test.

```{r, eval=F}
path = '/home/kenan/Documents/learning/masters/CUNY-SPS-Masters-DS/DATA_621/final/'

df <- read.csv(paste0(path,"test.csv"), header = T, stringsAsFactors = F, colClasses = c(fullVisitorId = "character"))

df_device <- paste("[", paste(df$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_geoNetwork <- paste("[", paste(df$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals <- paste("[", paste(df$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_trafficSource <- paste("[", paste(df$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)

df <- df %>%
    cbind(df_device, df_geoNetwork, df_totals, df_trafficSource) %>%
    select(-device, -geoNetwork, -totals, -trafficSource)

factorVars <- c("channelGrouping", "browser", "operatingSystem", "deviceCategory", "country")
df[, factorVars] <- lapply(df[, factorVars], as.factor)
df$transactionRevenue <- as.numeric(df$transactionRevenue)

numVars <- c("visits", "hits", "bounces", "pageviews", "newVisits")
df[, numVars] <- lapply(df[, numVars], as.integer)

df$visitStartTime <- as.POSIXct(df$visitStartTime, tz="UTC", origin='1970-01-01')
write.csv(df, 'DATA_621/final/test_clean.csv', row.names = FALSE)
```

The clean dataset has 903653 transactions and 55 columns, where each transaction is one visit to google store. The target variable is transactionRevenue, the amount of money GStore brought in from that transaction.

```{r}
path = '/home/kenan/Documents/learning/masters/CUNY-SPS-Masters-DS/DATA_621/final/'
df <- read.csv(paste0(path, 'train_clean.csv'))
df <- df %>% mutate(date=ymd(date),
                    day=weekdays(date), 
                    month=months(date), 
                    year=year(date))
```


```{r}
nan_count <- sum(!is.na(df$transactionRevenue))
nan_pct <- nan_count / nrow(df) * 100
total_rev <- sum(df$transactionRevenue, na.rm=T)
```

Only `r nan_count` sessions in the training set led to a transaction. This is only `r nan_pct`% of the observations, and that of course matches with the 98.7% missing values in the previous section. Altogether, the GStore made `r format_dollars(total_rev)` USD in revenues in that year. The missing transactionRevenue is replaced with 0 to avoid errors with NAN later on. This filling of missing value is acceptable since the lowest a revenue can go is 0.

## Exploratory Data Analysis

We being with looking at the transactionRevenue

```{r}
p1 <- df %>% filter(transactionRevenue > 0) %>% ggplot(aes(x=transactionRevenue)) + geom_histogram()
p2 <- df %>% filter(transactionRevenue > 0) %>% mutate(transactionRevenue=log(transactionRevenue)) %>% ggplot(aes(x=transactionRevenue)) + geom_histogram()
grid.arrange(p1, p2)
```

Most transactions generate no revenue hence the strongly skewed right tail. However, if we log the transactionRevenue it looks almost normal. Since the metrics of the competitation is in log scale, the transactionRevenue will be converted to log scale from here on.

```{r}
df <- df %>%
  mutate(transactionRevenue=log(transactionRevenue),
         transactionRevenue=replace_na(transactionRevenue,0))
gg_miss_var(df %>% filter(transactionRevenue > 0), show_pct = TRUE)
```


```{r}
df %>% filter(transactionRevenue > 0) %>% ggplot(aes(x=date, y=transactionRevenue)) + geom_line()
```

It looks like transactionRevenue doesn't so much variation through out the year

#### Sparse columns

Some columns have only a few values, those columns will be dropped as they carry no row-wise information.

```{r}
uniq_counts <-df %>% 
  select_if(negate(is.numeric)) %>% 
  apply(2, function(x) length(unique(x)))
uniq_cols <- names(uniq_counts[uniq_counts == 1])

uniq_cols
```

17 Columns contain only one value for all 900k rows. The columns below were all dropped by checks for their usefulness towards positive revenue , NA count, unique value counts and duplication of another column. 

- adwordsClickInfo.isVideoAd - NA (882166) and False ()
- adwordsClickInfo.adNetworkType - NA, and only "Google Search" for revenue greater than 0
- adwordsClickInfo.gclId - According to [this](https://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/66862 ) post gclId is a hash/encryption info for data is available in other features
- adwordsClickInfo.slot - 103 Top and 5362 RHS, the rest are NA
- adwordsClickInfo.page - NA and 1, for positive revenue
- campaignCode - NA for all value with positive revenue and "11251kjhkvahf" for other values
- isTrueDirect - NA and True for only a few rows
- visitId - sessionID already has this information
- visits - 1 for all positive revenue

```{r}
drop_cols = c('adwordsClickInfo.isVideoAd', 'adwordsClickInfo.adNetworkType', 'adwordsClickInfo.gclId', 'adwordsClickInfo.slot', 'adwordsClickInfo.page', 'campaignCode', 'isTrueDirect', 'visits', uniq_cols)

df <- df %>% select(-all_of(drop_cols))
```

After dropping those columns we are left with 30 columns.

#### Active users

```{r, echo=F}
session <- function(df, cols, cnt=10) {
    var_col <- enquo(cols)
    df %>% 
      count(!!var_col) %>%
      top_n(cnt, wt=n) %>%
    ggplot(aes_(x=var_col, y=~n, fill=var_col)) +
    geom_bar(stat='identity') +
    labs(x="", y="Sessions") +
    theme(legend.position="none")
}
revenue <- function(dataframe, cols, cnt=10) {
    var_col <- enquo(cols)
    df %>% 
      group_by(!!var_col) %>% 
      summarise(revenue=sum(transactionRevenue)) %>%
      filter(revenue >0) %>% 
      top_n(cnt, wt=revenue) %>%
    ggplot(aes_(x=var_col, y=~revenue, fill=var_col)) +
    geom_bar(stat='identity') +
    labs(x="", y="Revenue") +
    theme(legend.position="none")
}
```

An active user is someone that has visited the Google Merchandise Store

```{r}
p1 <- df %>% 
  group_by(date) %>% 
  summarise(dailySessions = n()) %>%
  ggplot(aes(x=date, y=dailySessions)) + 
  geom_line() + 
  geom_smooth(col='red') + 
  labs(x="", y="Sessions per Day")

p2 <- df %>% 
  group_by(date) %>% 
  summarise(revenue=sum(transactionRevenue)) %>%
  ggplot(aes(x=date, y=revenue)) + 
  geom_line() + 
  geom_smooth(col='red') + 
  labs(x="", y="Daily Revenue")

grid.arrange(p1, p2, nrow=2)
```

It looks like between Oct 2016 and Jan 2017 there was a peek in active users. 
The daily revenue slightly peeks around the same time then plateaus.


#### fullVistorId

FullVistorId is a unique identifier for each user of the Google Merchandise Store.

```{r}
df %>% group_by(fullVisitorId) %>% summarise(revenue=sum(transactionRevenue)) %>% filter(revenue > 0) %>% ggplot(aes(x=fullVisitorId, y=revenue)) + geom_point()
```

We can see most visitors spend a small amount while a few spend a lot, with one individual spending the most.

#### Datetime

The date column was parsed into days and months to see if there is any information on the session day/month and the transactionRevenue.

```{r}
month_order = c('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December')
p1 <- session(df, month, 12) + scale_x_discrete(limits=month_order)
p2 <- revenue(df, month, 12) + scale_x_discrete(limits=month_order)
grid.arrange(p1, p2)
```

The last quarter of the year has a lot of sessions and generates the most revenue. There is an increase in revenue from Jan to May with relatively the same amount of sessions. December generate the highest revenue which might be due to the holidays.

```{r}
day_order = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday')
p1 <- session(df, day, 7) + scale_x_discrete(limits=day_order)
p2 <- revenue(df, day, 7) + scale_x_discrete(limits=day_order)
grid.arrange(p1, p2)
```

Weekdays have the most activate sessions and the highest revenue. Revenue start low on Sunday, peaks on Monday and then slowly go down for the rest of the week. Sessions follow a more parabolic curve, peaking on Tuesday and tapering off on both sides.

#### Channel Grouping

Channels are the medium on how users came to the GStore.

```{r}
p1 <- session(df, channelGrouping, 7)
p2 <- revenue(df, channelGrouping, 7)
grid.arrange(p1, p2)
```

As you see in the plots, Organic Search and Social channels led to the most sessions; however social channels delivers very little revenue. Referral on the other hand delivers most revenues with a relatively small number of sessions. This makes sense since a referral is a session with a purpose while other channel groups could be casual browsing.

#### Device category

Device category are the hardware which people use to access the GStore, there are 3.

```{r}
p1 <- session(df, deviceCategory, 3)
p2 <- revenue(df, deviceCategory, 3)
#p3 <- df %>% filter(transactionRevenue > 0) %>% ggplot(aes(x=transactionRevenue, fill=deviceCategory)) + geom_histogram(position='identity', alpha=0.75, bins=50)
grid.arrange(p1, p2)
```

The main takeaway here is that mobile and tablet have a lot fewer sessions, and relatively low revenues per session when compared to desktop. Desktops produce the most sessions and revenue.

#### Operating System

An operating system is the next level from device category. There are 20 unique operating systems

"Windows"       "Macintosh"     "Linux"         "Android"       "iOS"           "Chrome OS"    
"BlackBerry"    "(not set)"     "Samsung"       "Windows Phone" "Xbox"          "Nintendo Wii" 
"Firefox OS"    "Nintendo WiiU" "FreeBSD"       "Nokia"         "NTT DoCoMo"    "Nintendo 3DS" 
"SunOS"         "OpenBSD"

with revenues; however most have low sessions and revenue so they are ignored. The "(no set)" category is most likely a missing value.

```{r}
p1 <- session(df, operatingSystem, 6)
p2 <- revenue(df, operatingSystem, 6)
grid.arrange(p1, p2)
```

What stands out is Macintosh has higher revenues than Windows with fewer sessions. In addition, it also confirms again that the mobile sessions (Android and iOS) have little revenues compared to the number of sessions. Macintosh and Windows are desktop operating systems which is inline with the device category analysis.

#### Browser

Web browsers are the last step to getting to the GStore. There are 54 web browsers from the popular chrome and safari to the less known such as Seznam and DoCoMo

```{r}
p1 <- session(df, browser, 5)
p2 <- revenue(df, browser, 5)
grid.arrange(p1, p2)
```

By far the most sessions and revenues come from the Chrome browser. Safari has a lot of sessions, more than FireFox but generates relatively low revenues. Firefox also has a healthy balance between sessions and revenue. 

#### Pageviews, Bounces and Hits

A pageview is each time a visitor views a page on your website.

```{r}
p1 <- df %>% filter(!is.na(df$pageviews) & pageviews <=30) %>% 
    ggplot(aes(x=pageviews)) +
    geom_histogram(binwidth=1) +
  geom_vline(xintercept=5, linetype='dashed', color='blue')

p2 <- df %>% filter(!is.na(df$pageviews) & pageviews <=30) %>% group_by(pageviews) %>% summarise(revenue=sum(transactionRevenue)) %>%
    ggplot(aes(x=pageviews, y=revenue)) +
    geom_col() + 
    geom_vline(xintercept=5, linetype='dashed', color='blue')

grid.arrange(p1, p2)
```


```{r}
p1 <- df %>% filter(!is.na(df$hits) & hits <=100) %>% 
    ggplot(aes(x=hits)) +
    geom_histogram(binwidth=1) +
  geom_vline(xintercept=5, linetype='dashed', color='blue') + 
  coord_cartesian(xlim=c(0,100)) +
  labs(title='Hits by Session verses Revenue') + 
  theme(plot.title = element_text(hjust = 0.5)) 

p2 <- df %>% filter(!is.na(df$hits) & hits <=100) %>% group_by(hits) %>% summarise(revenue=sum(transactionRevenue)) %>%
    ggplot(aes(x=hits, y=revenue)) +
    geom_col() + 
    geom_vline(xintercept=5, linetype='dashed', color='blue') + 
  coord_cartesian(xlim=c(0,100))

grid.arrange(p1, p2)
```

The distribution of pageviews is very right skewed. This makes sense since we mostly use a small number of sites. Most of the revenue is generated from sites with a small number of pageviews.

#### Country

There are 222 countries in the dataset which is most of the world. Most of them have near 0 sessions, mostly because of the lack of internet access.

```{r}
c1 <- session(df, country, 6)
c2 <- revenue(df, country, 6)
grid.arrange(c1, c2)
```

Since google is based in the US there is no surprise that the US has the highest session counts and largest revenue. Cananda and Venezuela follow very far behind.

#### adContent

AdContent as the name implies is the ad shown to the user at the time

```{r}
df %>% filter(transactionRevenue > 0) %>% drop_na(adContent) %>% group_by(adContent) %>% summarise(revenue=sum(transactionRevenue)) %>% ggplot(aes(x=adContent, y=revenue)) + geom_col() + labs(x='AdContent', y='Revenue') + coord_flip()
```

The results here are expected for sessions that produced positive revenue. Ads that were about Google Merchandise Collection generated the most revenue. Since this is a categorical variable the missing values are difficult to impute.

Look at the final dataframe

```{r}
skimr::skim(df)
```

Based on the EDA I would expected the most revenue from users with the following specifications

browser - Chrome
channelGrouping - Referral
operatingSystem - Macintosh
isMobile - False
deviceCategory - Desktop
continent - Americas
country - United States
city - New York
 
```{r}
corrplot(cor(df %>% filter(transactionRevenue>0) %>% select_if(is.numeric)))
```

 
##TODO: add correlation matrix
        
        
        
```{r}
df %>% group_by(operatingSystem) %>% summarise(total=sum(transactionRevenue), n=n()) %>% mutate(pct_rev=total/sum(total), pct_count=n/sum(n)) %>% arrange(desc(pct_rev)) %>% head(10)
```
        
## Modeling

The target in this dataset is a revenue so we will begin with a randomforest regression. Given the size of the dataset we will build a model on revenue greater than 0. The predictions will then be threshold to 0 for values between the 1st Quartile (17.03) and the Min (9.21) which is 13.12.

```{r, warning=F, error=F, message=F}
library(tidymodels)
library(randomForest)
library(vip)
```

#### Train test split

We begin by splitting the data into training and testing

```{r}
set.seed(42)
df_split <- df %>% 
  filter(transactionRevenue > 0) %>%
  initial_split(strata = transactionRevenue)

df_train <- training(df_split)
df_test <- testing(df_split)

set.seed(43)
df_folds <- bootstraps(df_train, strata = transactionRevenue, times=10)
```

#### Preprocessing

Certain features are selected based on the analysis above. Some of those features are then thresholded based on frequency below 1%. Other features are imputed with knn.
Step order [here](https://recipes.tidymodels.org/articles/Ordering.html)

```{r}
features <- c('transactionRevenue', 'channelGrouping', 'browser', 'operatingSystem', 'isMobile', 'deviceCategory', 'continent', 'subContinent', 'country', 'region', 'metro', 'city', 'networkDomain', 'hits', 'pageviews', 'source', 'medium', 'day', 'month')

df_rec <- recipe(transactionRevenue ~ ., data=df_train %>% select(all_of(features))) %>%
  step_novel(operatingSystem, country, city, browser, medium, networkDomain, metro, source, region, subContinent) %>%
  step_other(operatingSystem, country, city, browser, medium, networkDomain, subContinent, metro, threshold = 0.01) %>%
  step_impute_knn(pageviews, hits)

smp <- df_rec %>% prep() %>% bake(new_data=NULL)
```

#### Tuning 

Random Forest is a bagging technique that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset.‚Äù Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output. The greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.
source (https://medium.com/geekculture/xgboost-versus-random-forest-898e42870f30)[random_forest]

With the data preprocessed we build the random forest model with tuneable parameters

```{r}
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees=1500) %>%
  set_mode('regression') %>%
  set_engine('ranger')

rf_workflow <- workflow() %>%
  add_recipe(df_rec) %>%
  add_model(rf_spec)
```

#### Training

Using parallel processing 10 models are trained across a grid of 20 to find the best hyperparameters for the random forest model

```{r}
set.seed(44)
doParallel::registerDoParallel()
rf_tune <- tune_grid(rf_workflow, resamples = df_folds, grid=5)
```

#### Evaluation

The metric used in this competition is RMSE, so values corresponding to the lowest values are chosen

```{r}
show_best(rf_tune, metric='rmse')
autoplot(rf_tune)
```

Selecting the best parameters we train the full model on the trainined set

```{r}
rf_model <- rf_workflow %>%
  finalize_workflow(select_best(rf_tune, metric='rmse'))
```

When the tuning completed we can now train the model with the best fit parameters on the train set and test on the test set

#### Testing

```{r}
rf_fit <- last_fit(rf_model, df_split)
collect_metrics(rf_fit)
rmse_val <- collect_metrics(rf_fit) %>% filter(.metric == 'rmse') %>% select(.estimate)
```

Our model has a RMSE of 1.08 on the test set which is a little better than on the training set of `r rmse_val`.

Feature importance

```{r}
imp_spec <- rf_spec %>%
  finalize_model(select_best(rf_tune, metric = 'rmse')) %>%
  set_engine('ranger', importance='permutation')
```

```{r}
workflow() %>%
  add_recipe(df_rec) %>%
  add_model(imp_spec) %>%
  fit(df_test) %>%
  extract_fit_parsnip() %>%
  vip() + 
  labs(y='Importance', x='Features')
```

It looks like the number of hits and pageviews are among the most important features.

```{r}
model <- rf_fit$.workflow[[1]]
y_pred <- predict(model, df_test %>% select(features[2:19]))
rf_df <- data.frame(y_pred=y_pred$.pred, y_true=df_test$transactionRevenue)
rmse <- sqrt(mean((rf_df$y_true - rf_df$y_pred)^2))
p1 <- rf_df %>% 
  ggplot(aes(x=y_pred, y=y_true)) + 
    geom_point() +
    labs(title=paste0('Random Forest RMSE ',round(rmse,3))) + 
    coord_cartesian(xlim=c(14,21))
```

#### Generalized Linear Model

```{r}
glm1 <- glm(transactionRevenue ~., data=df_train %>% select(all_of(features)), family='gaussian')
y_pred <- predict(glm1, newdata = df_test %>% select(features[2:19]))
glm_df <- data.frame(y_pred=y_pred, y_true=df_test$transactionRevenue)
```

```{r}
rmse <- sqrt(mean((glm_df$y_true - glm_df$.pred)^2))
p2 <- glm_df %>% 
  ggplot(aes(x=.pred, y=y_true)) + 
    geom_point() +
    labs(title=paste0('GLM RMSE ',round(rmse,3))) + 
  coord_cartesian(xlim=c(14,21))
```

### Model Evaluation

The RMSE of the random forest model is lower than the GLM model therefore random forest is the better model.

```{r}
grid.arrange(p1, p2)
```

The random forest model is more narrow than the GLM model which explains why its RMSE is lower. The values for the random forest is also spread across while the GLM predictions are more lumped together in the middle.

### Competation testing

With the trained model we can now predict on the kaggle test set. The results will be uploaded to the leader board for evaluation

```{r, eval=F}
test <- read.csv(paste0(path, 'test_clean.csv')) %>% 
  mutate(date=ymd(date),
                    day=weekdays(date), 
                    month=months(date), 
                    year=year(date)) %>%
  select(features[2:19])
y_hat <- predict(glm1, new_data=test)
```