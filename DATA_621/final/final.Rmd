---
title: "Google Merchandise Store"
author: "Kenan Sooklall"
date: "10/13/2021"
output: html_document
---

```{r echo=F, warning=F, include=F}
library(tidyverse)
library(naniar)
library(jsonlite)
library(priceR)
library(lubridate)
library(gridExtra)
library(corrplot)
```

The dataset we are analyzing is a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset. The raw dataset contains 12 columns to predict the transaction revenue per customer. The outcome from this analysis will aid in better use of marketing budgets. There are 2 datasets, big and small. The small dataset is about 1 million transactions and the big one is about 10 times larger. Due to hardware constraint the small dataset is used in this report.

### Preprocessing

The data is a mix of csv and json where the json data are separate columns. That mismatch requires a lot of preprocessing to get the features and target that we need for analyzing. The json columns (device, geoNetwork, totals, trafficSource) are convert from str json into a json object. Then the json object is parsed with jsonlite into vectors. Those vectors are flattened and then join with the original dataframe. This preprocessing step is required for both the training and testing test.

```{r, eval=F}
path = '/home/kenan/Documents/learning/masters/CUNY-SPS-Masters-DS/DATA_621/final/'

df <- read.csv(paste0(path,"train.csv"), header = T, stringsAsFactors = F, colClasses = c(fullVisitorId = "character"))

df_device <- paste("[", paste(df$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_geoNetwork <- paste("[", paste(df$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals <- paste("[", paste(df$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_trafficSource <- paste("[", paste(df$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)

df <- df %>%
    cbind(df_device, df_geoNetwork, df_totals, df_trafficSource) %>%
    select(-device, -geoNetwork, -totals, -trafficSource)

factorVars <- c("channelGrouping", "browser", "operatingSystem", "deviceCategory", "country")
df[, factorVars] <- lapply(df[, factorVars], as.factor)
df$transactionRevenue <- as.numeric(df$transactionRevenue)

numVars <- c("visits", "hits", "bounces", "pageviews", "newVisits")
df[, numVars] <- lapply(df[, numVars], as.integer)

df$visitStartTime <- as.POSIXct(df$visitStartTime, tz="UTC", origin='1970-01-01')
write.csv(df, 'DATA_621/final/train_clean.csv', row.names = FALSE)
```

The clean dataset has 903653 transactions and 55 columns, where each transaction is one visit to google store. The target variable is transactionRevenue, the amount of money GStore brought in from that transaction.

```{r}
path = '/home/kenan/Documents/learning/masters/CUNY-SPS-Masters-DS/DATA_621/final/'
df <- read.csv(paste0(path, 'train_clean.csv'))
df <- df %>%
  mutate(date=ymd(date))
```


```{r}
nan_count <- sum(!is.na(df$transactionRevenue))
nan_pct <- nan_count / nrow(df) * 100
total_rev <- sum(df$transactionRevenue, na.rm=T)

df <- df %>%
  mutate(transactionRevenue=replace_na(transactionRevenue, 0))
```

Only `r nan_count` sessions in the training set led to a transaction. This is only `r nan_pct`% of the observations, and that of course matches with the 98.7% missing values in the previous section. Altogether, the GStore made `r format_dollars(total_rev)` USD in revenues in that year. The missing transactionRevenue is replaced with 0 to avoid errors with NAN later on. This filling of missing value is acceptable since the lowest a revenue can go is 0.

## Exploratory Data Analysis

We being with looking at the transactionRevenue

```{r}
p1 <- df %>% filter(transactionRevenue > 0) %>% ggplot(aes(x=transactionRevenue)) + geom_histogram()
p2 <- p1 + scale_x_log10(labels=scales::dollar_format())
grid.arrange(p1, p2)
```

Most transactions generate no revenue hence the strongly skewed right tail. However, if we log the transactionRevenue it looks almost normal,

```{r}
df <- df %>%
  mutate(transactionRevenue=log(transactionRevenue + 0.01))
gg_miss_var(df %>% filter(transactionRevenue > 0), show_pct = TRUE)
```

#### Sparse columns

Some columns have only a few values, those columns will be dropped as they carry no row-wise information.

```{r}
uniq_counts <-df %>% 
  select_if(negate(is.numeric)) %>% 
  apply(2, function(x) length(unique(x)))
uniq_cols <- names(uniq_counts[uniq_counts == 1])

uniq_cols
```

17 Columns contain only one value for all 900k rows. The columns below were all dropped by checks for their usefulness towards positive revenue , NA count, unique value counts and duplication of another column. 

adwordsClickInfo.isVideoAd - NA (882166) and False ()
adwordsClickInfo.adNetworkType - NA, and only "Google Search" for revenue greater than 0
adwordsClickInfo.gclId - According to [this](https://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/66862 ) post gclId is a hash/encryption info for data is available in other features
adwordsClickInfo.slot - 103 Top and 5362 RHS, the rest are NA
adwordsClickInfo.page - NA and 1, for positive revenue
campaignCode - NA for all value with positive revenue and "11251kjhkvahf" for other values
isTrueDirect - NA and True for only a few rows
visitId - sessionID already has this information
visits - 1 for all positive revenue

```{r}
drop_cols = c('adwordsClickInfo.isVideoAd', 'adwordsClickInfo.adNetworkType', 'adwordsClickInfo.gclId', 'adwordsClickInfo.slot', 'adwordsClickInfo.page', 'campaignCode', 'isTrueDirect', 'visits')

df <- df %>% 
  select(-c(uniq_cols, drop_cols))
```

After dropping those columns we are left with 30 columns.

#### Active users

```{r, echo=F}
session <- function(df, cols, cnt=10) {
    var_col <- enquo(cols)
    df %>% 
      count(!!var_col) %>% 
      top_n(cnt, wt=n) %>%
    ggplot(aes_(x=var_col, y=~n, fill=var_col)) +
    geom_bar(stat='identity') +
    labs(x="", y="Sessions") +
    theme(legend.position="none")
}


revenue <- function(dataframe, cols, cnt=10) {
    var_col <- enquo(cols)
    df %>% 
      group_by(!!var_col) %>% 
      summarise(revenue=sum(transactionRevenue)) %>%
      filter(revenue >0) %>% 
      top_n(cnt, wt=revenue) %>%
    ggplot(aes_(x=var_col, y=~revenue, fill=var_col)) +
    geom_bar(stat='identity') +
    labs(x="", y="Revenue") +
    theme(legend.position="none")
}
```

An active user is someone that has visited the Google Merchandise Store

```{r}
p1 <- df %>% 
  group_by(date) %>% 
  summarise(dailySessions = n()) %>%
  ggplot(aes(x=date, y=dailySessions)) + 
  geom_line() + 
  geom_smooth(col='red') + 
  labs(x="", y="Sessions per Day")

p2 <- df %>% 
  group_by(date) %>% 
  summarise(revenue=sum(transactionRevenue)) %>%
  ggplot(aes(x=date, y=revenue)) + 
  geom_line() + 
  geom_smooth(col='red') + 
  labs(x="", y="Daily Revenue")

p3 <- df %>% 
  group_by(date) %>% 
  summarise(revenue=sum(-log10(transactionRevenue + 0.01))) %>%
  ggplot(aes(x=date, y=revenue)) + 
  geom_line() + 
  geom_smooth(col='red') + 
  labs(x="", y="Daily Log Revenue")

grid.arrange(p1, p2, p3, nrow=3)
```

It looks like between Oct 2016 and Jan 2017 there was a peek in active users
The daily revenue is almost completely linear and some what independant to the number of active users which seems incorrect. When we transform the revenue though log we see the affects of more active users.

#### Channel Grouping

Channels are the medium on how users came to the GStore.

```{r}
p1 <- session(df, channelGrouping, 12)
p2 <- revenue(df, channelGrouping, 12)
grid.arrange(p1, p2)
```

As you see in the plots, Organic Search and Social channels led to the most sessions; however social channels delivers very little revenue. Referral on the other hand delivers most revenues with a relatively small number of sessions. This makes sense since a referral is a session with a purpose while other channel groups could be casual browsing.

#### Device category

Device category are the hardware which people use to access the GStore, there are 3.

```{r}
p1 <- session(df, deviceCategory, 3)
p2 <- revenue(df, deviceCategory, 3)
#df %>% filter(transactionRevenue > 0) %>% ggplot(aes(x=transactionRevenue, fill=deviceCategory)) + geom_histogram(position='identity', alpha=0.75, bins=50) + scale_x_log10(labels=scales::dollar_format())
grid.arrange(p1, p2)
```

The main takeaway here is that mobile and tablet have a lot fewer sessions, and relatively low revenues per session when compared to desktop. Desktops produce the most sessions and revenue.

#### Operating System

An operating system is the next level from device category. There are 20 unique operating systems

"Windows"       "Macintosh"     "Linux"         "Android"       "iOS"           "Chrome OS"    
"BlackBerry"    "(not set)"     "Samsung"       "Windows Phone" "Xbox"          "Nintendo Wii" 
"Firefox OS"    "Nintendo WiiU" "FreeBSD"       "Nokia"         "NTT DoCoMo"    "Nintendo 3DS" 
"SunOS"         "OpenBSD"

with revenues; however most have low sessions and revenue so they are ignored. The "(no set)" category is most likely a missing value.

```{r}
p1 <- session(df, operatingSystem, 6)
p2 <- revenue(df, operatingSystem, 6)
grid.arrange(p1, p2)
```

What stands out is Macintosh has higher revenues than Windows with fewer sessions. In addition, it also confirms again that the mobile sessions (Android and iOS) have little revenues compared to the number of sessions. Macintosh and Windows are desktop operating systems which is inline with the device category analysis.

#### Browser

Web browsers are the last step to getting to the GStore. There are 54 web browsers from the popular chrome and safari to the less known such as Seznam and DoCoMo

```{r}
p1 <- session(df, browser, 5)
p2 <- revenue(df, browser, 5)
grid.arrange(p1, p2)
```

By far the most sessions and revenues come from the Chrome browser. Safari has a lot of sessions, more than FireFox but generates relatively low revenues. Firefox also has a healthy balance between sessions and revenue. 

#### Pageviews, Bounces and Hits

A pageview is each time a visitor views a page on your website.

```{r}
p1 <- df %>% filter(!is.na(df$pageviews) & pageviews <=30) %>% 
    ggplot(aes(x=pageviews)) +
    geom_histogram(binwidth=1)

p2 <- df %>% filter(!is.na(df$pageviews) & pageviews <=30) %>% group_by(pageviews) %>% summarise(revenue=sum(transactionRevenue)) %>%
    ggplot(aes(x=pageviews, y=revenue)) +
    geom_col()
grid.arrange(p1, p2)
```

The distribution of pageviews is very right skewed. This makes sense since we mostly use a small number of sites. Most of the revenue is generated from sites with a small number of pageviews.

#### Country

There are 222 countries in the dataset which is most of the world. Most of them have near 0 sessions, mostly because of the lack of internet access.

```{r}
c1 <- session(df, country, 6)
c2 <- revenue(df, country, 6)
grid.arrange(c1, c2)
```

Since google is based in the US there is no surprise that the US has the highest session counts and largest revenue. Cananda and Venezuela follow very far behind.

#### adContent

AdContent as the name implies is the ad shown to the user at the time

```{r}
df %>% filter(transactionRevenue > 0) %>% drop_na(adContent) %>% group_by(adContent) %>% summarise(revenue=sum(transactionRevenue)) %>% ggplot(aes(x=adContent, y=revenue)) + geom_col() + labs(x='AdContent', y='Revenue') + coord_flip()
```

The results here are expected for sessions that produced positive revenue. Ads that were about Google Merchandise Collection generated the most revenue. Since this is a categorical variable the missing values are difficult to impute.

Look at the final dataframe

```{r}
skimr::skim(df)
```

Columns still worth looking at 
 - keyword
 
```{r, eval=F}
train <- df[,which(sapply(df, is.numeric))] %>% select(-c('fullVisitorId', 'visitId')) %>% mutate(across(everything(), ~replace_na(0)))
correlation <- cor(train)
corrplot.mixed(correlation, tl.col = 'black', tl.pos = 'lt', number.cex= 11/ncol(raw))
```
 
Based on the EDA I would expected the most revenue from users with the following specifications

browser - Chrome
channelGrouping - Referral
operatingSystem - Macintosh
isMobile - False
deviceCategory - Desktop
continent - Americas
country - United States
city - New York
 
## Modeling

```{r}
library(tidymodels)
library(randomForest)
```

I add 0.01 to the transactionRevenue to avoid -inf when taking the log10

```{r}
set.seed(42)
df_split <- df %>% 
  mutate(trevenue = log10(transactionRevenue + 0.01)) %>%
  initial_split(strata = trevenue)

df_train <- training(df_split)
df_test <- training(df_split)

set.seed(43)
df_folds <- vfold_cv(df_train, v=2, strata = trevenue)
```

```{r}
df_rec <- recipe(trevenue ~ channelGrouping + browser + operatingSystem + 
                   isMobile + deviceCategory + continent + country + 
                   city + pageviews, data=df_train) %>%
  step_other(operatingSystem, country, city, browser, threshold = 0.01) %>%
  step_impute_knn(pageviews)

smp <- df_rec %>% prep() %>% bake(new_data=NULL)
```

Tuning the model

```{r}
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees=10) %>%
  set_mode('regression') %>%
  set_engine('ranger')

rf_workflow <- workflow() %>%
  add_recipe(df_rec) %>%
  add_model(rf_spec)
```


```{r}
set.seed(44)
doParallel::registerDoParallel(cores=8)
rf_tune <- tune_grid(rf_workflow, resamples = df_folds, grid=5)
```

