---
title: "Final Proposal"
author: "Kenan Sooklall, Jack Wright, Rachel Greenlee, Sean Connin, Daniel Moscoe, Stefano Biguzzi, Mustafa Telab"
date: "10/22/2021"
output:
  tufte::tufte_html: default
  tufte::tufte_handout: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## GStore, Revenue per Customer Prediction

For our final project, we will be looking at the Kaggle competition `Google Analytics Customer Review Prediction` where we will *predict revenue per customer* for the Google store.

In this competition, we're challenged to analyze the Google Merchandise Store customer dataset to predict revenue per customer. Understanding patterns in customer revenue can help guide marketing strategy, as well as lend insight into how customer behavior changes over time.

## Why is this important?

It is a widely held belief that 80% of a business' revenue comes from 20% of their customers, also known as the *80/20 rule*. If we are able to predict revenue per customer, then we should be able to identify this most important 20% of a stores customers.  We selected a Kaggle competition so that, in addition to the feedback gathered from our model's performance on a holdout set, we will also be able to learn from how other teams approached this problem. Seeing others' work after we complete ours will let us know which of our methods were most successful, and what strategies we might consider in future work.

With our analysis, we hope to discover the characteristics of the highest revenue-generating customers.


## Background

Here are a few features unique to predicting future spending that we may choose to look at.

**Activity Analysis**

As highlighted in the paper [Predictive Modeling Using Transactional Data](https://www.capgemini.com/wp-content/uploads/2017/07/Predictive_Modeling_Using_Transactional_Data.pdf), activity analysis sets a time frame for which a customer can be defined as inactive and can be used to model attrition of a business. Maybe we can leverage this into a dummy variable to predict future spending. 

**Cohort and Trend Analysis**:

There seems to be some trend in future revenue prediction between *attriters* and *high transactors*. It is recommended to look at customers who have not spent during a previous period (for example, the previous three months) likelihood to spend moving forward. We might be able to investigate this same trend with customers attrition of visiting the site as well.  

**Stock vs Flow Variables**

Stock variables are attributes of the customer that do not change, such as race, sex, location etc.. wheras flow variables are attributes that change across time, such as monthly average transaction count. For example, it is recommended to use the mean monthly transaction count and its directionality as opposed to trying to model off the time series of the monthly transaction count. 


**Predictions on Groups with Varying Amounts of Data**:

In the paper [Predictive Profiles for Transaction Data using Finite Mixture
Models](http://www.datalab.uci.edu/papers/profiles.pdf), The authors bring up the problem of making predictions on transactors with low amounts of data (say one purchase only). They recommend using *Bayesian estimation for learning predictive profiles*. In essence, we can create different predictive profiles, or models, depending on how much data we have about the transactor. For example, a transactor with a low amount of information (one purchase) would have a more general model to the data applied, while as a frequent transactor could have a much more data driven model. 

This technique is generally used in machine learning, but we don't see why we couldn't apply it to regression.

## Methodology:

We will begin with feature engineering with some of the approaches mentioned above.
We will start with both linear regression models and Random Forest decision trees to model the data. Based on our outcome from those models we can extend our analysis to different models or fine-tune those models. The final model will be submitted to Kaggle to see how it stacks up against the community.
