Week 7:
Both decision tree and random forest are supervised machine learning algorithms. Decision trees can be infered as a subset of random forest, since Random forest is a collection decision trees.

decision tree
It handles data accurately and works best for a linear pattern. It handles large data easily and takes less time.

Advantages
Easy
Transparent process
Handle both numerical and categorical data
Larger the data, the better the result
Speed
Disadvantages
May overfit
Pruning process large
Optimization unguaranteed
Complex calculations
Deflection high


Random forest
The basic difference being it does not rely on a singular decision. It assembles randomized decisions based on several decisions and makes the final decision based on the majority.

Advantages
Powerful and highly accurate
No need to normalizing
Can handle several features at once
Run trees in parallel ways
Disadvantages
They are biased to certain features sometimes
Slow
Can not be used for linear methods
Worse for high dimensional data

Week 8:

AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well. AdaBoost algorithms can be used for both classification and regression problem.

From sklearn:
An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.


reference:
https://en.wikipedia.org/wiki/AdaBoost
https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe
